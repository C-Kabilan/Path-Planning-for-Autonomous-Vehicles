This project presents a simulation-based approach to autonomous path planning in grid environments using a Deep Q-Learning algorithm. The objective is to train an intelligent agent to navigate from a start point to a goal location while avoiding obstacles, simulating basic behaviour of autonomous robots in uncertain environments. The environment is modelled as a 5x5 grid with static obstacles and a fixed start and goal location. The methodology involves the use of Deep Reinforcement Learning (DRL), where the agent learns optimal navigation strategies through interactions with the environment. A neural network is used as a function approximator to estimate Q-values for each possible action at a  given state. The agent receives rewards based on its movements: positive for reaching the goal, negative for hitting obstacles, and small penalties to discourage wandering. Over multiple episodes, the agent improves its decision-making ability using experience replay and epsilon greedy exploration to balance learning and exploration. The model was implemented entirely in Python using TensorFlow and executed in a Google Colab environment, requiring no external hardware or advanced simulation tools. After  training over several episodes, the agent successfully learned to navigate the grid while avoiding obstacles and finding the shortest path to the goal. The results demonstrate the effectiveness of Deep Q-Learning in solving simple path planning problems and serve as a foundational experiment for more complex autonomous systems. This project illustrates how AI-based control systems can be applied to real-world robotics and planning challenges in a resource-efficient and accessible manner.
